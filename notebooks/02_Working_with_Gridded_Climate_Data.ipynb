{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be74f339-99e2-4d63-8a2c-36af922a67e2",
   "metadata": {},
   "source": [
    "# M2.2 - Working with Gridded Climate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91647cb-5949-42d9-b373-b51b9bfb436b",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "- [Conceptual and computational constraints](#Conceptual-and-computational-constraints)\n",
    "- [A terrestrial water storage (TWS) time series](#A-terrestrial-water-storage-(TWS)-time-series)\n",
    "- [Thinking about multi-dimensional arrays](#Thinking-about-multi-dimensional-arrays)\n",
    "  - [Updating coordinate systems](#Updating-coordinate-systems)\n",
    "- [Indexing a multi-dimensional array](#Indexing-a-multi-dimensional-array)\n",
    "- [Aggregating along the axis of a multi-dimensional array](#Aggregating-along-the-axis-of-a-multi-dimensional-array)\n",
    "- [Resampling a time series](#Resampling-a-time-series)\n",
    "  - [Handling missing data](#Handling-missing-data)\n",
    "- [Visualizing variation over space](#Visualizing-variation-over-space)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979c87b-18c6-4ca1-84ca-4a9bb63f2180",
   "metadata": {},
   "source": [
    "## Conceptual and computational constraints\n",
    "\n",
    "As computer technology improves and more satellite-based datasets are available, the size, complexity, and frequency of climate data are increasing rapidly. This can lead to two problems for researchers and resources managers looking to use climate data. First, it can be difficult to conceptualize the size and scale of some climate datasets, leading to issues with data and project management (Jain et al. 2022). Second, it can be difficult to process and analyze climate datasets that are sometimes too large to fit into computer memory or too complex for tractable computation.\n",
    "\n",
    "In this lesson, we'll explore how to manage some of these issues. Because we're learning, we'll use a dataset that isn't as large or complex as others we might encounter, but it should serve as a good illustration of more difficult datasets and how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bc6ce-9274-4335-a724-55729604fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import earthaccess\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "auth = earthaccess.login(strategy = 'netrc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f21a7-307a-4d56-9751-b943c72c09d6",
   "metadata": {},
   "source": [
    "## A terrestrial water storage (TWS) time series\n",
    "\n",
    "To gain more experience working with gridded climate data, and particularly a time series of gridded climate data, we'll use a terrestrial water storage (TWS) dataset produced using data from two satellite missions: the Gravity Recovery and Climate Experiment (GRACE) mission and its successor, the GRACE Follow-On (GRACE-FO) mission.\n",
    "\n",
    "TWS includes all water on and under the land surface, in the form of snowpack, surface streams and reservoirs, and groundwater. [The TWS dataset we'll be working with from the GRACE and GRACE-FO missions](https://podaac.jpl.nasa.gov/dataset/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06.1_V3) has already been converted to *monthly anomalies.*\n",
    "\n",
    "&#x1F449; [Read more about GRACE/GRACE-FO data here.](https://grace.jpl.nasa.gov/about/faq/)\n",
    "\n",
    "Recall that *anomalies* are one way of representing variability around a long-term mean. They tell us something about how a particular data point (a particular day, month, or year) compares over time; e.g., is this an especially dry year?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de943f1-a70a-4bcd-9bf8-89c8e6d4a5cb",
   "metadata": {},
   "source": [
    "#### &#x1F3AF; Best Practice\n",
    "\n",
    "*We're about to download some raw data! You know what to do.* Create a new folder called `GRACE` inside the `data_raw` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2d398-4c3d-42c7-af71-70faedacf3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(short_name = 'TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06.1_V3')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809b2ea-a68e-4d12-a255-a5dc62c47910",
   "metadata": {},
   "source": [
    "This dataset has been prepared as a single netCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4ea78-3d47-41dd-9604-7a256a834163",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.download(results, 'data_raw/GRACE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a292caf-e5e7-442b-bd87-e09537c8a019",
   "metadata": {},
   "source": [
    "**You may get an error message when trying to download this dataset because of a problem with NASA's servers.** If that's the case, note that there is a single URL for this dataset (included in the output above), which you can use to download the data directly. It should be:\n",
    "\n",
    "- [https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06.1_V3/GRCTellus.JPL.200204_202311.GLO.RL06.1M.MSCNv03CRI.nc](https://archive.podaac.earthdata.nasa.gov/podaac-ops-cumulus-protected/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06.1_V3/GRCTellus.JPL.200204_202311.GLO.RL06.1M.MSCNv03CRI.nc) (Click to download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc11fe-c6ad-4a9b-928d-1b48697277ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('data_raw/GRACE/GRCTellus.JPL.200204_202311.GLO.RL06.1M.MSCNv03CRI.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f83a0-b3c7-4b28-b64f-6dbf40a6a1b7",
   "metadata": {},
   "source": [
    "The GRACE/GRACE-FO data already represent *anomalies,* i.e., a change in total water storage relative to some time period (or \"epoch\"). What time period is that? We can find that out from the **file-level attributes** of this netCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e833d33-4e46-42ac-b70b-eec127bf3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.attrs['time_mean_removed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7add60-7081-477f-a265-ea797be7a09a",
   "metadata": {},
   "source": [
    "We can also verify this by looking at a time series of the global average in monthly TWS anomalies. For the period 2004-2009, those anomalies appear to be centered on zero, which is what we would expect since the mean of that period was subtracted from the data.\n",
    "\n",
    "We also see a long-term downward trend. What might explain this? The global loss of land ice is probably the chief factor driving this loss. The most Assessment Report (AR6) from the Intergovernmental Panel on Climate Change (IPCC) estimates that glacial and ice-sheet melt since between 2006 and 2018 accounts for almost 2 centimeters of sea level rise (IPCC 2021, Chapter 9), which is consistent with what we see here (the vertical axis of our plot is also in centimeters). The warming of Earth's atmosphere also plays a role, as warmer air can hold more water vapor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ffffc0-cc45-4cb2-920e-126d40b7e9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lwe_thickness'].mean(['lon','lat']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f7713-cb61-47be-a0af-e0c77817c3a0",
   "metadata": {},
   "source": [
    "The anomaly in the time series around 2017-2018 is due to the fact that GRACE's mission ended in October 2017 and GRACE-FO was not launched until May 2018; hence, there are no data values for those months.\n",
    "\n",
    "We can look at more than a time series: this is a spatial dataset, as well. Let's take a look at the global TWS anomalies for first date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e440e-bfdd-43b2-b0b8-a7c828ad5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lwe_thickness'][0].plot(vmin = -200, vmax = 200, cmap = 'RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcacc9-91cc-492d-89e1-3f2e34369e57",
   "metadata": {},
   "source": [
    "In the above plot, which depicts the monthly TWS anomaly for April 2002, cool colors show a positive TWS anomaly (i.e., more water than usual for April) and warmer colors show a negative TWS anomaly (i.e., less water than usual for April)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151cc38-a61a-4639-9b49-1472301969ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Thinking about multi-dimensional arrays\n",
    "\n",
    "**Let's re-open our dataset using `open_mfdataset()`.** Recall that we use the `open_mfdataset()` from `xarray` whenever we want to open multiple files as if they were a single dataset. Here, we have only one file, but we'll pretend it is multiple files because it contains a global TWS anomaly image for multiple dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0ea44-2413-4207-9128-e006bf82d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset('data_raw/GRACE/GRCTellus.JPL.200204_202311.GLO.RL06.1M.MSCNv03CRI.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd2a9a-e794-41a6-8634-785e4c9645c5",
   "metadata": {},
   "source": [
    "As we've seen, netCDF4 and HDF5 files can be represented as `xarray` Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc1a93-2643-4c8f-ae9a-fc5679b6d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944c1bc-86d9-46cb-9722-87be15238641",
   "metadata": {},
   "source": [
    "A Dataset can contain more than one Variable, each represented as a multi-dimensional array.\n",
    "\n",
    "There are different ways that we can represent arrays in Python. When we use `xarray`, the variables are represented as DataArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32c160-dafa-4981-b109-ae1d0e782f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds['lwe_thickness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a298fe4-6b4d-4b01-b1e9-ed955a845a0c",
   "metadata": {},
   "source": [
    "The `xarray` DataArray is usually just a special kind of NumPy array. But when we use `xarray.open_mfdataset()` we get a new type of array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ea5d3-ac04-40ce-b7c5-a768b72721ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds['lwe_thickness'].data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad044bbe-c7cc-4f36-90ac-ccf3a48720ae",
   "metadata": {},
   "source": [
    "**The `dask` library is automatically used to represent our underlying multi-dimensional array.** This is because the use of `open_mfdataset()` implies we have a potentially large multi-dimensional array to work with, because we are opening multiple files.\n",
    "\n",
    "**But by using `xarray.open_mfdataset()` to stack multiple files together in a time series, we've also gained a new way of thinking about our data.** Each individual file represented essentially a 2D image: the TWS anomaly on a latitude-longitude grid. With multiple image dates stacked together, we obtain a 3D **data cube,** with X (longitude), Y (latitude), and time axes. `xarray` Datasets can show a helpful illustration when we use them inside a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4fae6-28d4-4591-96d1-7bcc64593308",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lwe_thickness'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8096e-24f8-4b09-bb85-7a815d07ba5d",
   "metadata": {},
   "source": [
    "When analyzing data cubes, we're typically trying to answer one of these questions:\n",
    "\n",
    "1. How does a climate variable in one or more locations vary over time?\n",
    "1. How does a climate variable at a certain time vary over space?\n",
    "1. How does climate variability (variation over time) compare across space?\n",
    "\n",
    "The first two questions don't require a data cube to answer, but we often have to merge different datasets together in order to answer them. We might consider ourselves lucky to start with a data cube, in that case, because we have all of the data in one place. These questions require us to subset or *index* a multi-dimensional array.\n",
    "\n",
    "The third question can only be answered if we start with a data cube; it's a question that requires us to aggregate or *collapse* an axis of our data cube."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be7029-4fe1-4f4c-999c-d0827dd0d184",
   "metadata": {},
   "source": [
    "### Updating coordinate systems\n",
    "\n",
    "Recall that `xarray` Datasets have coordinate systems, which encode the spatial information of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc8518-930a-45fd-aee3-c1d721eb5143",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5523d51e-3462-4aa0-a57e-b8b8118ab7d5",
   "metadata": {},
   "source": [
    "We can see that our data are on a half-degree latitude-longitude grid; i.e., each pixel is 0.5 degrees by 0.5 degrees.\n",
    "\n",
    "You may have noticed that while the latitudes, or `lat` values, span -90 degrees to 90 degrees, the longitude, or `lon` values, span 0 to 360 degrees. We're used to talking about longitudes that span -180 degrees (West) longitude to 180 degrees (East) longitude. How can we fix this?\n",
    "\n",
    "#### &#x1F6A9; <span style=\"color:red\">Pay Attention</red>\n",
    "\n",
    "**Let's change our `lon` values so they are easier to work with.** We'll subtract 180 degrees from the coordinates so that the smallest longitude value is -180 and the largest value, which used to be approximately 360, is now 180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c321238-04ff-4432-bf17-ff032f923bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this only once!\n",
    "ds.coords['lon'] = ds.coords['lon'] - 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b5669-75e7-4390-9212-43a551744d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee936d9e-5c19-498b-917e-f3b04fe9be7b",
   "metadata": {},
   "source": [
    "**Coordinates are like labels for the data points in our data cube, while dimensions are the axes of the data cube.** We can apply multiple labels to the same dimension. For example, we have both a dimension named `time` and a coordinate set named `time`. The `time` coordinate describes the precise moment in time down to the nanosecond (ns), but we could also label the time points of our dataset by the year, month, season, or day.\n",
    "\n",
    "**Let's add a new `year` coordinate to our dataset, so we can identify individual data points by a common year.** We first extract the numeric year from the `\"time\"` coordinate. Then, we use [the `assign_coords()` method](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.assign_coords.html) of an `xarray.Dataset` to create new coordinate labels along an existing dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a176a-a1db-418b-a701-bd25cd32f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the dates from the \"time\" coordinate\n",
    "dates = ds.coords['time'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e89b40-70f3-4f14-a0a1-d1db69ba101d",
   "metadata": {},
   "source": [
    "Take a look at what `dates` contains.\n",
    "\n",
    "```python\n",
    "dates\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e013da-5f69-4c26-ad8b-8f302edc2d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each \"datetime64[ns]\" object to a 4-digit year\n",
    "years_list = []\n",
    "for each in dates:\n",
    "    years_list.append(int(str(each)[0:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086fd89-d2d5-4704-b8d6-5e0ad9042c6d",
   "metadata": {},
   "source": [
    "Take a look at what `years_list` contains.\n",
    "\n",
    "```python\n",
    "years_list\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc860d2-b465-4011-9069-1c7f11199d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new coordinate (\"year\") along an existing dimension (\"time\")\n",
    "ds = ds.assign_coords(year = ('time', years_list))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d1629-1ec6-4def-a64d-60d3fcc30a0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Indexing a multi-dimensional array\n",
    "\n",
    "We can slice our data cube a number of different ways to answer different questions. For example, what do the past 20 years of TWS anomalies look like in the Western U.S., given the multi-decadal drought the region has experienced?\n",
    "\n",
    "Let's start with looking at the pixel that contains Sacramento, CA. Slicing our array at this single pixel amounts to taking a thin stip along the time axis; as illustrated below, we end up with what is essentially a 1D array of 220 values (for 220 months of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23435a53-7c94-49d2-be09-70552df50072",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lwe_thickness'].sel(lon = -121.75, lat = 38.25).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c4dc0-d86f-4b03-8d78-993ccc1bb1f2",
   "metadata": {},
   "source": [
    "This is equivalent to slicing our array along the X and Y (longitude and latitude) axes, as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890eb508-ff50-4f3b-8bee-fd75abf6091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three dimensions: (time, lat, lon)\n",
    "ds['lwe_thickness'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f3e3b-d735-45a7-9bdc-ce4cc693333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first pixel along the \"lat\" dimension and the first pixel along the \"lon\" dimension\n",
    "ds['lwe_thickness'][:,0,0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf589a3-1069-4543-83d8-3ecd9df592dd",
   "metadata": {},
   "source": [
    "The data could be represented as a time series, since we have one value over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c651002-f43a-4c98-a522-5d36774b852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lwe_thickness'].sel(lon = -121.75, lat = 38.25).plot(figsize = (12, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ed1a9-ff86-4088-aa52-183fac552fd3",
   "metadata": {},
   "source": [
    "**If we wanted to look at multiple pixels representing a region of interest, we're still slicing our array along the time axis, but we end up a smaller data cube.**\n",
    "\n",
    "Recall that the built-in `slice()` function can be used in combination with the `sel()` method of `xarray` to select a region of interest. In this case, our region is 10 pixels wide by 10 pixels tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480183f-79ca-4564-ae3e-0fb4d59c5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us = ds['lwe_thickness'].sel(lon = slice(-124, -114), lat = slice(32, 42))\n",
    "west_us.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df2604-8ad9-4b34-ba33-16a7c8aa302e",
   "metadata": {},
   "source": [
    "There's no easy way to visualize the values of a 3D data cube, so when we ask `xarray` to plot the data, it just shows us a histogram, essentially pooling all the values from the data cube together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f7ccc-b173-4b81-a8b0-867219c664ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781492c1-ba88-42f4-91bf-c0636e7c529e",
   "metadata": {},
   "source": [
    "Of course, we can always plot a single time step by subsetting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6e097-3b5e-4124-ac06-faaed41727a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.sel(time = '2003-01-01', method = 'nearest').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db043bc-0669-4d6b-a758-24d41530e991",
   "metadata": {},
   "source": [
    "From the way this image looks (and if we examine the underlying array values), we can infer that while this latitude-longitude dataset has a 0.5-degree spatial resolution, the values are repeated in some areas because [the actual spatial resolution of the data is closer to 300 km](https://grace.jpl.nasa.gov/about/faq/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75b960-53f2-4dda-8aa7-0ff5b45b0638",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Aggregating along the axis of a multi-dimensional array\n",
    "\n",
    "As the histogram above suggests, we need a way of transforming our data cube in order to better visualize the data and answer some of our questions.\n",
    "\n",
    "**What's the average TWS anomaly in our study area in each month?** This can be answered by averaging over the spatial domain. We can visualize this as taking an average of each 10-by-10 pixel slice in our data cube, for each monthly time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46e662-8ec1-4691-a2eb-09f59ac939ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b8c8e-877e-4f3f-9e40-a0397f355c16",
   "metadata": {},
   "source": [
    "If we specify we want to average over one of our spatial axes, we go from a 3D array to a 2D array; i.e., **we collapsed one of our axes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73fa649-a0d5-4445-906f-4bfdb2a40944",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.mean('lat').data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e610300-5863-4dee-a4e3-d1d59db3d088",
   "metadata": {},
   "source": [
    "Specifically, we lost the `\"lat\"` dimension, as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca24f3c-2db2-4ae8-92f5-71bf753d1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.mean('lat').dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f332e0-f3d5-42ff-97a8-f05d211bd73a",
   "metadata": {},
   "source": [
    "Let's average over two of our spatial axes: averaging over latitude *and* longitude. This means we go from a 3D array to a 1D array; although it is depicted, below, as a 2D array, we have a trivial axis of length one (1), so we really have just a 1D sequence of 220 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48288fa2-6062-4733-a129-e643bef9082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.mean(['lat', 'lon']).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febecebc-1db2-4eda-b5ca-b56f9d43cac2",
   "metadata": {},
   "source": [
    "And since we have a 1D time series of TWS anomalies, we can visualize them as a line plot. From this plot, we can see that the multi-decadal drought in the Western U.S. has gotten worse in recent years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c2b873-b664-4cb7-8bae-687239ac56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.mean(['lat', 'lon']).plot(figsize = (12, 6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d141d60-cddd-4507-ac93-a6d40dacc805",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resampling a time series\n",
    "\n",
    "Another common data analysis task we might want to perform with time-series data is to re-aggregate the data; for example, can we calculate the mean *annual* TWS anomaly, to better characterize wet and dry years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84810c53-f376-416f-9fbf-23bbd27d6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbba35-3222-4924-b227-231e1f473c8a",
   "metadata": {},
   "source": [
    "[The `resample()` method of an `xarray.DataArray`](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.resample.html) is the best way to aggregate time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176a193-8ee7-451a-bf57-1222d6dff409",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us_annual = west_us.resample(time = 'YS').mean()\n",
    "west_us_annual.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1bceef-655a-424b-8c3c-fc9aea74ae11",
   "metadata": {},
   "source": [
    "The syntax `time = 'YS'`, above, indicates that the *start of the year* (abbreviated `YS`) should be used as the resampling frequency; i.e., calculate the mean for each year. This syntax comes from the `pandas` library. \n",
    "\n",
    "&#x1F449; [**Read more about `resample()` and resampling frequencies here.**](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60fd29-4f19-4c46-9fa0-7b423437d627",
   "metadata": {},
   "source": [
    "We should also use `resample()` for certain calculations we perform, such as calculating time trends, where the units of a trend are denominated by the unit of time (e.g., \"per day\" or \"per year\"). This is because `coarsen()` doesn't change the unit of time; in this case, it doesn't change the units of time from days to years.\n",
    "\n",
    "So, in general, with time-series axes, it's better to use the `resample()` method of a DataArray.\n",
    "\n",
    "But note that when we resample along a given dimension, not all the coordinate labels are preserved. In this case, `xarray` doesn't know how to assign a `year` label to the resampled coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2bf6d-8922-4478-bb36-bd704d091a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We lost the \"year\" coordinate\n",
    "west_us_annual.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26643a82-1f08-4b60-8e89-6a28ea444edf",
   "metadata": {},
   "source": [
    "#### &#x1F6A9; <span style=\"color:red\">Pay Attention</red>\n",
    "\n",
    "But in this particular case, since we are interested in aggregating by year, anyway, we don't have to use `resample()`; we can use `groupby()` to indicate we want to take the mean within each `\"year\"` coordinate label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e775453-ff8b-4ede-b6bd-aac2c09fb739",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us_annual = west_us.groupby('year').mean()\n",
    "west_us_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db556059-dd94-4d69-80fc-8d08bfc83773",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "\n",
    "With satellite-based datsets like this TWS dataset, there is always a possibility of missing data. For example, there is a gap in 2017 and 2018 when the GRACE mission ended, before the GRACE-FO satellite was launched. There are also missing months in 2002, before the launch of GRACE in March, and the 2023 GRACE-FO data do not include December 2023, because it is still being processed.\n",
    "\n",
    "&#x1F449; **Note that, because you're accessing these data from NASA's cloud, you may find that more recent data (with fewer missing months) are available!**\n",
    "\n",
    "Below, we count how many months are available in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae3ad63-f52c-4abc-b884-a961dc489664",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = ds.groupby('year').count()\n",
    "\n",
    "# Get a value (the count) in each year for an arbitrary pixel, skipping the first year (2002)\n",
    "counts['lwe_thickness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d9fdc-7c84-4fc0-9a36-1bb61d27f1b0",
   "metadata": {},
   "source": [
    "**However, we're getting a count of the months in each pixel!** We just want the count for one (arbitrary) pixel because, with this modeled TWS product, the data availability is the same for every pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff8c3f-5108-47ee-b52a-1772ef590683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count at an arbitrary pixel: the top-left (0,0) corner\n",
    "counts_by_year = counts['lwe_thickness'][:,0,0].values\n",
    "counts_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d907473-d26a-4b2c-8fd4-07881046356e",
   "metadata": {},
   "source": [
    "It's apparent we're missing several months of data, particularly in 2017 and 2018 (only 5 months available each year) between the GRACE and GRACE-FO missions.\n",
    "\n",
    "We should first make sure we have at least some data in every year. Below, we can see our time axis has 21 elements (21 years). This lines up with our monthly counts when we skip the first year (2002)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8f8a2-3119-4895-b963-f55ca7e00ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us_annual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cbf493-014e-43ea-b596-a1bae6e3f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_year.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58ff95-b590-4603-9fc7-2a5205126aab",
   "metadata": {},
   "source": [
    "**Let's impose a rule: We need at least 9 months of data in each year for a reliable annual average.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56e4d-e476-40b6-b26f-f1bdbd76fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = counts_by_year < 9\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbace1-3b71-41d0-889b-ba86c03c3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us_annual[mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb025f7-1c88-473c-b1de-4f063b22e1ef",
   "metadata": {},
   "source": [
    "It's worth remembering that a related fix for this problem, if all the valid data points were consecutive, would be to take a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb877f-4ec7-4ffb-a617-df95619f61eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A related fix, if the valid data points were all consecutive\n",
    "west_us_annual = west_us_annual.sel(year = slice(2003, 2023))\n",
    "west_us_annual.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db9004-6633-4cb3-b8b0-82a139764df7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647259df-87bd-41b8-92e0-b990b9c2dff1",
   "metadata": {},
   "source": [
    "Aggregating time intervals is just one of the many neat things an `xarray.DataArray` can do! [Check out the `xarray` documentation to learn more.](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e87b08-c0f5-4a34-9772-eba48ec6fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us_annual.mean(['lat','lon']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb029a-c2eb-46ff-af3c-2010d987910a",
   "metadata": {},
   "source": [
    "We can observe a general downward trend in the data, which is consistent with the decades-long drought the region has experienced (Liu et al. 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d979d10-2990-4d35-8f2a-2a9304918513",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualizing variation over space\n",
    "\n",
    "In the previous example, we resampled our data over the temporal axis (over time). But our dataset has spatial coordinates, too! We often want to visualize how temporal variation compares between different locations. This is simply a matter of resampling our data cube a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291ec10-6181-40a0-a702-6970bb832fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us_annual.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41adb10-8406-460f-8466-27d91bf46863",
   "metadata": {},
   "source": [
    "We could calculate a simple inter-annual mean. This collapses our data cube to two spatial dimensions, latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7fa27-7f59-4956-8ef8-c0444624e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us_annual.mean('year').data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de116d2-7623-4f13-b436-1940cc260599",
   "metadata": {},
   "source": [
    "**However, it'd be much more interesting if we could calculate pixel-level trends in the annual TWS anomalies.**\n",
    "\n",
    "Because we're working with `xarray`, a highly efficient and mature Python library for working with multi-dimensional arrays, it's a good idea to check to see if the functionality we want is already built into the `xarray.DataArray` class. Indeed, there are two functions that might help here:\n",
    "\n",
    "- `DataArray.curvefit()` [(Documentation)](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.curvefit.html#xarray.DataArray.curvefit)\n",
    "- `DataArray.polyfit()` [(Documentation)](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.polyfit.html#xarray.DataArray.polyfit)\n",
    "\n",
    "Either one can be used to fit a function to a set of data points. However, we want to use `polyfit()` today because we are fitting a simple function (a linear trend) that should have [a closed-form solution](https://en.wikipedia.org/wiki/Closed-form_expression). A \"degree 1\" polynomial is a straight line because, in the function involved, $y(x) = mx + b$, $x$ is raised to the power of 1: $x^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb88d7-703c-4073-9f85-d4773c34a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = west_us_annual.polyfit('year', deg = 1)\n",
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec58206-f81a-49b3-bea7-af58100534ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit['polyfit_coefficients']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c87c7-d843-46c3-b235-80fc3fefd4a7",
   "metadata": {},
   "source": [
    "**We obtained another data cube!** This data cube has an axis called **degree** with two (2) elements. While we asked for a 1-degree polynomial fit, recall that we need two numbers to describe a line: the y-intercept and the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51411a0d-5722-4958-b458-a892daef8bd2",
   "metadata": {},
   "source": [
    "#### &#x1F6A9; <span style=\"color:red\">Pay Attention</red>\n",
    "\n",
    "Another thing to note about our data cube is that we have a trend line (y-intercept and slope) for each of 400 pixels. Those 400 linear regressions seemed really fast! \n",
    "\n",
    "**Actually, we haven't computed the linear regressions yet. This is because we loaded the TWS data into the computer's memory using `xr.open_mfdataset()`.** This function automatically signals to `xarray` that we may be working with a very large dataset! Consequently, `xarray` doesn't actually do any computation until we explicitly ask it to.\n",
    "\n",
    "So, what happened when we typed the following?\n",
    "\n",
    "```python\n",
    "fit = west_us_annual.polyfit('year', deg = 1)\n",
    "```\n",
    "\n",
    "What happened is that we obtained a **representation** of the computation we wanted `xarray` to perform. We called this representation `fit` and it looks like a data cube because that is what the result of the computation will look like when it is performed.\n",
    "\n",
    "**When we want to actually comptue the results, we need to call the `compute()` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57564b-0c52-4bb2-875f-bf76ffa26d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a representation of the computation we want to do\n",
    "fit = west_us_annual.polyfit('year', deg = 1)\n",
    "\n",
    "# Actually run the computation\n",
    "results = fit.compute()\n",
    "\n",
    "# Look at the coefficients\n",
    "results['polyfit_coefficients']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f8a6c4-9499-4adb-8dba-4d83952c2759",
   "metadata": {},
   "source": [
    "**There are two ways to ensure that computations on `xarray` Datasets and DataArrays actually run when you want them to:**\n",
    "\n",
    "- Call the `compute()` method, as in the example above.\n",
    "- Or, try to access the `values` property, as in the example below.\n",
    "\n",
    "```python\n",
    "fit['polyfit_coefficients'].values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333381dc-cb33-480c-b860-671477aafd35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Here are the key Python tools and techniques to remember from this lesson:\n",
    "\n",
    "- You can use `xarray.open_mfdataset()` to open multiple files as a single `xarray` Dataset. For example, you might write\n",
    "  `data = xarray.open_mfdataset(\"precip_in_year_Y*.nc\")` to open a series of annual precipitation data files.\n",
    "- The attributes (or metadata) of any file opened in `xarray` can be accessed through: `data.attrs`\n",
    "- `xarray` Datasets are also sometimes called **data cubes.** You can aggregate over one or more of **axes** (or \"dimensions\") of the data cube using methods like `mean()`; for example, to average over the two spatial dimensions of longitude and latitude: `data['variable'].mean(['lon','lat'])`.\n",
    "- To slice a data cube along one or more axes, use the `sel()` method, for example: `data['variable'].sel(lon = -121.75, lat = 38.25)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2f97f-1612-40bd-b443-c4278bf478e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "- IPCC. *Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change.* 2021. Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA, 2391 pp. doi:10.1017/9781009157896\n",
    "- Jain S, Mindlin J, Koren G, Gulizia C, Steadman C, Langendijk GS, Osman M, Abid MA, Rao Y, Rabanal V. 2022. [Are we at risk of losing the current generation of climate researchers to data science?](https://doi.org/10.1029/2022AV000676) *AGU Advances.*\n",
    "- Liu, P.-W., J. S. Famiglietti, A. J. Purdy, K. H. Adams, A. L. McEvoy, J. T. Reager, R. Bindlish, D. N. Wiese, C. H. David, and M. Rodell. 2022. [Groundwater depletion in California’s Central Valley accelerates during megadrought.](https://www.nature.com/articles/s41467-022-35582-x) Nature Communications 13 (1):7825.\n",
    "- xarray.DataArray (xarray Documentation). [https://docs.xarray.dev/en/stable/generated/xarray.DataArray.html](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.html) Accessed: February 5, 2024.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Computation on `xarray` Datasets and DataArrays](https://docs.xarray.dev/en/stable/user-guide/computation.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
