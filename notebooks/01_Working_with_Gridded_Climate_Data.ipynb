{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be74f339-99e2-4d63-8a2c-36af922a67e2",
   "metadata": {},
   "source": [
    "# M2.1 - Working with Gridded Climate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91647cb-5949-42d9-b373-b51b9bfb436b",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "- Lorem ipsum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979c87b-18c6-4ca1-84ca-4a9bb63f2180",
   "metadata": {},
   "source": [
    "## Conceptual and computational constraints\n",
    "\n",
    "As computer technology improves and more satellite-based datasets are available, the size, complexity, and frequency of climate data are increasing rapidly. This can lead to two problems for researchers and resources managers looking to use climate data. First, it can be difficult to conceptualize the size and scale of some climate datasets, leading to issues with data and project management (Jain et al. 2022). Second, it can be difficult to process and analyze climate datasets that are sometimes too large to fit into computer memory or too complex for tractable computation.\n",
    "\n",
    "In this lesson, we'll explore how to manage some of these issues. Because we're learning, we'll use a dataset that isn't as large or complex as others we might encounter, but it should serve as a good illustration of more difficult datasets and how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bc6ce-9274-4335-a724-55729604fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import earthaccess\n",
    "\n",
    "auth = earthaccess.login(strategy = 'netrc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f21a7-307a-4d56-9751-b943c72c09d6",
   "metadata": {},
   "source": [
    "## A terrestrial water storage (TWS) time series\n",
    "\n",
    "To gain more experience working with gridded climate data, and particularly a time series of gridded climate data, we'll use a terrestrial water storage (TWS) dataset from the Global Land Data Assimilation System (GLDAS), which is a global version of the NLDAS we are already familiar with.\n",
    "\n",
    "TWS includes all water on and under the land surface, in the form of snowpack, surface streams and reservoirs, and groundwater. [The TWS dataset we'll be working with from GDLAS](https://podaac.jpl.nasa.gov/dataset/TELLUS_GLDAS-NOAH-3.3_TWS-ANOMALY_MONTHLY) has already been converted to *monthly anomalies.*\n",
    "\n",
    "Recall that *anomalies* are one way of representing variability around a long-term mean. They tell us something about how a particular data point (a particular day, month, or year) compares over time; e.g., is this an especially dry year?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de943f1-a70a-4bcd-9bf8-89c8e6d4a5cb",
   "metadata": {},
   "source": [
    "### Performing our skills: Handling raw data\n",
    "\n",
    "*We're about to download some raw data! You know what to do.* Create a new folder called `GLDAS` inside the `data_raw` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bfb8d1-4eea-42a2-aac5-1839175fe95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name = 'TELLUS_GLDAS-NOAH-3.3_TWS-ANOMALY_MONTHLY')\n",
    "\n",
    "# About 20 MB in total, which might take 1-2 minutes\n",
    "earthaccess.download(results, 'data_raw/GLDAS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f7713-cb61-47be-a0af-e0c77817c3a0",
   "metadata": {},
   "source": [
    "Let's take a look at one of the datasets we just downloaded. Recall that when we use `glob.glob()`, we can easily get a list of files, but they may not be chronological (or even alphanumeric) order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e440e-bfdd-43b2-b0b8-a7c828ad5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob('data_raw/GLDAS/*.nc')\n",
    "files.sort()\n",
    "\n",
    "ds = xr.open_dataset(files[0]) # Just the first file\n",
    "ds['TWS_monthly'].plot(cmap = 'RdYlBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcacc9-91cc-492d-89e1-3f2e34369e57",
   "metadata": {},
   "source": [
    "In the above plot, which depicts the TWS anomaly for April 2002, cool colors show a positive TWS anomaly (i.e., more water than usual for April) and warmer colors show a negative TWS anomaly (i.e., less water than usual for April)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d579897-1c3a-4a5e-817f-ab5072b70173",
   "metadata": {},
   "source": [
    "### Getting a TWS time series\n",
    "\n",
    "As we've seen previously, the `xarray` library has a function, `open_mfdataset()` that can be used to open multiple files as a single dataset. The files may represent multiple pieces of a dataset along *any axis;* in this case, each file represents a different part of the time axis. `xarray` can figure out automatically what order the files should go in because netCDF4 files are structured in a way that explicitly defines X, Y, and time axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec8dd3-595a-49b8-9ee4-8a3ad9b6e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tws_anomaly = xr.open_mfdataset('data_raw/GLDAS/*.nc')\n",
    "tws_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b151cc38-a61a-4639-9b49-1472301969ea",
   "metadata": {},
   "source": [
    "## Thinking about multi-dimensional arrays\n",
    "\n",
    "As we've seen, netCDF4 and HDF5 files can be represented as `xarray` Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc1a93-2643-4c8f-ae9a-fc5679b6d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tws_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944c1bc-86d9-46cb-9722-87be15238641",
   "metadata": {},
   "source": [
    "A Dataset can contain more than one Variable, each represented as a multi-dimensional array.\n",
    "\n",
    "There are different ways that we can represent arrays in Python. When we use `xarray`, the variables are represented as DataArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32c160-dafa-4981-b109-ae1d0e782f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tws_anomaly['TWS_monthly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a298fe4-6b4d-4b01-b1e9-ed955a845a0c",
   "metadata": {},
   "source": [
    "The `xarray` DataArray is usually just a special kind of NumPy array. But when we use `xarray.open_mfdataset()` we get a new type of array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ea5d3-ac04-40ce-b7c5-a768b72721ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tws_anomaly['TWS_monthly'].data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad044bbe-c7cc-4f36-90ac-ccf3a48720ae",
   "metadata": {},
   "source": [
    "**The `dask` library is automatically used to represent our underlying multi-dimensional array.** This is because the use of `open_mfdataset()` implies we have a potentially large multi-dimensional array to work with, because we are opening multiple files.\n",
    "\n",
    "**But by using `xarray.open_mfdataset()` to stack multiple files together in a time series, we've also gained a new way of thinking about our data.** Each individual file represented essentially a 2D image: the TWS anomaly on a latitude-longitude grid. With multiple image dates stacked together, we obtain a 3D **data cube,** with X (longitude), Y (latitude), and time axes. `xarray` Datasets can show a helpful illustration when we use them inside a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4fae6-28d4-4591-96d1-7bcc64593308",
   "metadata": {},
   "outputs": [],
   "source": [
    "tws_anomaly['TWS_monthly'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8096e-24f8-4b09-bb85-7a815d07ba5d",
   "metadata": {},
   "source": [
    "When analyzing data cubes, we're typically trying to answer one of these questions:\n",
    "\n",
    "1. How does a climate variable in one or more locations vary over time?\n",
    "1. How does a climate variable at a certain time vary over space?\n",
    "1. How does climate variability (variation over time) compare across space?\n",
    "\n",
    "The first two questions don't require a data cube to answer, but we often have to merge different datasets together in order to answer them. We might consider ourselves lucky to start with a data cube, in that case, because we have all of the data in one place. These questions require us to subset or *index* a multi-dimensional array.\n",
    "\n",
    "The third question can only be answered if we start with a data cube; it's a question that requires us to aggregate or *collapse* an axis of our data cube."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d1629-1ec6-4def-a64d-60d3fcc30a0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Indexing a multi-dimensional array\n",
    "\n",
    "We can slice our data cube a number of different ways to answer different questions. For example, what do the past 20 years of TWS anomalies look like in the Western U.S., given the multi-decadal drought the region has experienced?\n",
    "\n",
    "Let's start with looking at the pixel that contains Sacramento, CA. Slicing our array at this single pixel amounts to taking a thin stip along the time axis; as illustrated below, we end up with what is essentially a 1D array of 220 values (for 220 months of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23435a53-7c94-49d2-be09-70552df50072",
   "metadata": {},
   "outputs": [],
   "source": [
    "tws_anomaly['TWS_monthly'].sel(lon = -121.5, lat = 38.5).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf589a3-1069-4543-83d8-3ecd9df592dd",
   "metadata": {},
   "source": [
    "The data could be represented as a time series, since we have one value over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c651002-f43a-4c98-a522-5d36774b852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tws_anomaly['TWS_monthly'].sel(lon = -121.5, lat = 38.5).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ed1a9-ff86-4088-aa52-183fac552fd3",
   "metadata": {},
   "source": [
    "**If we wanted to look at multiple pixels representing a region of interest, we're still slicing our array along the time axis, but we end up a smaller data cube.**\n",
    "\n",
    "Recall that the built-in `slice()` function can be used in combination with the `sel()` method of `xarray` to select a region of interest. In this case, our region is 10 pixels wide by 10 pixels tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480183f-79ca-4564-ae3e-0fb4d59c5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us = tws_anomaly['TWS_monthly'].sel(lon = slice(-124, -114), lat = slice(32, 42))\n",
    "west_us.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df2604-8ad9-4b34-ba33-16a7c8aa302e",
   "metadata": {},
   "source": [
    "There's no easy way to visualize the values of a 3D data cube, so when we ask `xarray` to plot the data, it just shows us a histogram, essentially pooling all the values from the data cube together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f7ccc-b173-4b81-a8b0-867219c664ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75b960-53f2-4dda-8aa7-0ff5b45b0638",
   "metadata": {},
   "source": [
    "## Aggregating along the axis of a multi-dimensional array\n",
    "\n",
    "As the histogram above suggests, we need a way of transforming our data cube in order to better visualize the data and answer some of our questions.\n",
    "\n",
    "**What's the average TWS anomaly in our study area in each month?** This can be answered by averaging over the spatial domain. We can visualize this as taking an average of each 10-by-10 pixel slice in our data cube, for each monthly time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46e662-8ec1-4691-a2eb-09f59ac939ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b8c8e-877e-4f3f-9e40-a0397f355c16",
   "metadata": {},
   "source": [
    "If we specify we want to average over one of our spatial axes, we go from a 3D array to a 2D array; i.e., **we collapsed one of our axes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73fa649-a0d5-4445-906f-4bfdb2a40944",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.mean('lat').data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f332e0-f3d5-42ff-97a8-f05d211bd73a",
   "metadata": {},
   "source": [
    "In this case, we want to average over two of our spatial axes: averaging over latitude *and* longitude. This means we go from a 3D array to a 1D array; although it is depicted, below, as a 2D array, we have a trivial axis of length one (1), so we really have just a 1D sequence of 220 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48288fa2-6062-4733-a129-e643bef9082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_us.mean(['lat', 'lon']).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2f97f-1612-40bd-b443-c4278bf478e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "- Jain S, Mindlin J, Koren G, Gulizia C, Steadman C, Langendijk GS, Osman M, Abid MA, Rao Y, Rabanal V. 2022. [Are we at risk of losing the current generation of climate researchers to data science?](https://doi.org/10.1029/2022AV000676) *AGU Advances.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
