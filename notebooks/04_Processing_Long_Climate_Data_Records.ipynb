{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdebed81-cb5e-4626-b15f-d3a9e5216441",
   "metadata": {},
   "source": [
    "# M2.4 - Processing Long Climate Data Records Concurrently\n",
    "\n",
    "*Part of:* [**Computational Climate Science**](https://github.com/OpenClimateScience/M2-Computational-Climate-Science) | **Previous Lesson** | **Next Lesson**\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- [Resource limitations in computing](#Resource-limitations-in-computing)\n",
    "  - [CPU-bound problems](#CPU-bound-problems)\n",
    "- [Concurrent processing for large climate datasets](#Concurrent-processing-for-large-climate-datasets)\n",
    "- [Computing PET using Hargreaves equation](#Computing-PET-using-Hargreaves-equation)\n",
    "  - [Computing top-of-atmosphere (TOA) radiation](#Computing-top-of-atmosphere-(TOA)-radiation)\n",
    "  - [Well-documented functions](#Well-documented-functions)\n",
    "  - [Vectorized functions](#Vectorized-functions)\n",
    "  - [Deriving variables from `xarray` coordinates](#Deriving-variables-from-xarray-coordinates)\n",
    "- [Applying a function to independent chunks](#Applying-a-function-to-independent-chunks)\n",
    "  - [Mapping an arbitrary function](#Mapping-an-arbitrary-function)\n",
    "- [Profiling computational resources](#Profiling-computational-resources)\n",
    "  - [Measuring the wall time of a task with `timeit`](#Measuring-the-wall-time-of-a-task-with-timeit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e31ff-0c6a-4471-9de5-ae2870bc764a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the previous lesson, we discussed how a simple bucket model can be used to quantify the difference between water supply (precipitation) and water loss (potential evapotranspiration or PET). The ratio of these two quantities is also useful as an index of how much of the water loss is replenished by precipitation:\n",
    "$$\n",
    "\\text{Percentage replenished} \\approx 100\\times \\frac{\\text{Precipitation}}{\\text{PET}}\n",
    "$$\n",
    "\n",
    "**The method for calculating PET that we will use is [the Hargreaves method](https://www.fao.org/4/X0490E/x0490e07.htm#minimum%20data%20requirements) (Allen et al. 2000), because it only requires temperature data.** We'll use temperature data from MERRA-2 to calculate PET. Then, we'll use precipitation data from CHIRPS, again, to derive our hydrologic drought index.\n",
    "\n",
    "**While there are many sources of PET data, we're going to calculate PET on our own so that we can get more experience working with large climate datasets.** Along the way, we'll learn how large climate datasets can be processed **concurrently,** which can help to address two common problems:\n",
    "\n",
    "1. The entire dataset is too large to load into memory all at once;\n",
    "2. Data processing can be time-consuming, either because the dataset is so large or because the computations are complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8dbd3d-611a-491a-a502-8054975592a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resource limitations in computing\n",
    "\n",
    "Generally, the bigger the dataset, the more computational resources are required to analyze it. But exactly what resources are needed depends on both the data and the kind of analysis we want to perform.\n",
    "\n",
    "**In computing problems, there are three major kinds of resource limitations or *bottlenecks,* i.e., limiting factors to running a computer algorithm:**\n",
    "\n",
    "1. **Read and write speed from a file system**\n",
    "2. **Computer memory**\n",
    "3. **Central processing unit (CPU) clock speed (e.g., 3 GHz)**\n",
    "   \n",
    "A bottleneck of **Type 1** occurs when we have either very large datasets or slow file-system read-write speeds. The speed of reading and writing from a file system (or hard disk) depends on the medium; solid-state drives are generally faster than spinning disk hard drives. If the drive is a network attached storage (NAS) device instead of the hard-drive on your computer, then the speed of the network connection is also part of Type 1 bottlenecks. **Problems that are limited by a Type 1 bottleneck are called I/O-Bound (Input/Output-Bound).**\n",
    "\n",
    "A bottleneck of **Type 2** can occur if the dataset is very large and we try to store it all in memory at once, or if our analysis generates too much data in memory. Of course, memory is finite, so data either fits in memory or it doesn't. If our computer program is very sophisticated, it can offload some data stored in memory onto the computer's hard disk. This is called *swapping* and it is extremely slow. Hence, if you are running out of computer memory, your program may not stop due to a lack of memory, but it will slow down severely as it tries to juggle data between memory and hard disk. **Problems that are limited by a Type 2 bottleneck are called Memory-Bound.**\n",
    "\n",
    "A bottleneck of **Type 3** has a lot less to do with the data and more to do with the algorithm we're running. If we're reading in a huge dataset and just doing a simple unit conversion (for example, multiplying the data by 1000 and then saving it back to disk), then CPU clock speed probably isn't an issue: computers can multiply numbers very fast. But exactly how fast depends on how fast the CPU is. **Problems that are limited by a Type 3 bottleneck are called CPU-Bound.**\n",
    "\n",
    "### CPU-bound problems\n",
    "\n",
    "Historically, Type 3 bottlenecks have received the most attention. Improvements in the manufacturing process for CPUs have led to faster and faster chips. Gordon Moore was one of the first to notice the rate of this upward trend, and **Moore's Law** has been an article of faith in the industry for a long time: the tendency for CPU clock speeds to double every 2 years (Moore 1965).\n",
    "\n",
    "[But there are recent signs that this rate of doubling may be slowing down.](https://www.tomshardware.com/tech-industry/semiconductors/intels-ceo-says-moores-law-is-slowing-to-a-three-year-cadence-but-its-not-dead-yet) There are several reasons for this that are beyond the scope of this lesson (Bohr 2007). A major reason is the problem of heat dissipation. Trying to maintain the same rate of growth in transistors has required making transistors smaller. But the smaller they get, the hotter they get when electricity flows through them. Modern chip design is primarily concerned with trying to keep things from melting!\n",
    "\n",
    "However, if we combine multiple low-power CPUs together, we can actually get better performance than from a single, high-power CPU. Consider the figure below. With a single CPU, it is only possible to process data in a Sequential or Concurrent scheme. Sequential processing means that only a single task can be worked on before switching to another task.\n",
    "\n",
    "![](./assets/M2_concurrency.jpg)\n",
    "\n",
    "*Image by [Kevin Wahome](https://kwahome.medium.com/concurrency-is-not-parallelism-a5451d1cde8d)*\n",
    "\n",
    "In a **Concurrent scheme,** computers can seamlessly switch between tasks so fast that it appears as if multiple tasks, or **threads,** are being worked on simultaneously. **Concurrency** or **multi-threading** is how single CPUs have allowed us to do multiple tasks for the first few decades of the personal computer. **To get faster computers and mobile phones today, we are now using multiple, low-power CPUs to work on independent tasks simultaneously.** This is the **Parallel scheme.** \n",
    "\n",
    "**Today, we'll see how multiple CPUs can be used to break a problem down into smaller parts that can be executed simultaneously.** Some of the tools we're working make it so easy to use a Concurrent or Parallel processing scheme that it can be hard to tell the difference between the two. So, in this lesson, we'll use the terms \"Concurrent processing\" or \"Concurrency\" to refer to both the Concurrent and Parallel processing schemes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e3fc433-cf29-496a-8394-569ddb943d83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Concurrent processing for large climate datasets\n",
    "\n",
    "As we've seen previously, we can use `earthaccess` to download [MERRA-2](https://gmao.gsfc.nasa.gov/reanalysis/MERRA-2/) data from NASA EarthData Search. We'll be using the daily, aggregated data we used before, with the `short_name` `\"M2SDNXSLV\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5919795-51a6-46a3-9a3a-6c7ff2c26dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot\n",
    "\n",
    "auth = earthaccess.login()\n",
    "\n",
    "results = earthaccess.search_data(\n",
    "    short_name = 'M2SDNXSLV',\n",
    "    temporal = (\"2024-01-01\", \"2024-05-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a970837-b200-4fa4-94b0-22dc21473597",
   "metadata": {},
   "source": [
    "#### &#x1F3AF; Best Practice\n",
    "\n",
    "**Remember: We want to make sure we don't accidentally change our raw data, so these data should be downloaded to a folder reserved for raw data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfa0d2-0be4-4aff-b334-3d70f1fa8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could take about 1 minute on a broadband connection\n",
    "earthaccess.download(results, 'data_raw/MERRA2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a456c07-d7df-4e5b-bc79-b553ff9cf534",
   "metadata": {},
   "source": [
    "Once again, we'll use `xr.open_mfdataset()` to open our collection of files as a single `xarray.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc92eb-03fc-45ae-ae2c-45c6853da409",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset('./data_raw/MERRA2/*2024*.nc4')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f493631-9cf7-458e-9e17-96af0202aced",
   "metadata": {},
   "source": [
    "The MERRA-2 data variables we are interested in are:\n",
    "\n",
    "- `T2MMAX`, the maximum daily temperature (degrees C)\n",
    "- `T2MMEAN`, the mean daily temperature (degrees C)\n",
    "- `T2MMIN`, the minimum daily temperature (degrees C)\n",
    "\n",
    "Note that we have 122 days of data, so the resulting data cube has a time axis of 122 daily time steps. `xarray` has automatically broken our dataset into equal-sized **chunks** that could be processed independently.\n",
    "\n",
    "&#x1F449; In `xarray`, a **chunk** (also called a **block**) is a piece of our dataset: a defined subset along one or more axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63610a8-2d1f-4f01-b806-477800cb934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['T2MMEAN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e333b-8b62-438a-bf6b-e4f7394895c0",
   "metadata": {},
   "source": [
    "**The size and shape of the chunks are important if we are going to use concurrency.** Consider, for example, if we wanted to calculate long-term trends. With the chunks we currently have, we could not calculate trends because each chunk contains only one time step.\n",
    "\n",
    "We could try using [the `chunks` argument of `open_mfdataset()`](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html) to specify that chunks should have 122 elements along the `time` axis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0825fa6-7004-4630-863e-344c29394f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"chunks\" argument tells xarray what size the chunks should be on one or more axes\n",
    "ds = xr.open_mfdataset('./data_raw/MERRA2/*2024*.nc4', chunks = {'time': 122})\n",
    "ds['T2MMEAN'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30601f15-fdde-46a8-b542-fb7767cd4c54",
   "metadata": {},
   "source": [
    "However, it's clear that didn't work; each chunk still only has one time step.\n",
    "\n",
    "#### &#x1F6A9; <span style=\"color:red\">Pay Attention</red>\n",
    "\n",
    "**This is because the `chunks` argument is evaluated separately for each file.** `xr.open_mfdataset()` opens multiple files and combines them into a single dataset but, in this case, because each file represents a different time step, it can't create chunks that span multiple files.\n",
    "\n",
    "Alternatively, we can tell `xarray` how big each chunk should be along the `lat` and `lon` axes, because this doesn't require spanning multiple files. Below, we specify chunk sizes that result in just 4 chunks for every file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ca154-8395-4bc7-849f-8bffd9f46abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset('./data_raw/MERRA2/*2024*.nc4', chunks = {'lat': 182, 'lon': 288})\n",
    "ds['T2MMEAN'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5bcc1-c2fd-4357-8556-acb78131531e",
   "metadata": {},
   "source": [
    "**If we really needed each chunk to contain the entire `time` axis (122 time steps), we would need to re-chunk the data *after* reading in all the files.** We can do this using [the `chunk()` method of an `xarray.Dataset` or `xarray.DataArray`.](https://docs.xarray.dev/en/stable/generated/xarray.DataArray.chunk.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7879c-79fa-4397-a5b2-ffdd09aacb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Re-chunking the data *after* loading is generally inefficient, but might be necessary; \n",
    "#    give example of \"what if\" we were interested in calculating trends\n",
    "\n",
    "ds = xr.open_mfdataset('./data_raw/MERRA2/*2024*.nc4')\n",
    "ds = ds.chunk({'time': 122})\n",
    "ds['T2MMEAN'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95989278-6a28-4450-83cd-3ab50638b440",
   "metadata": {},
   "source": [
    "#### &#x1F3AF; Best Practice\n",
    "\n",
    "In general, it's best to use the `chunks` argument because re-chunking the data is inefficient. However, in cases where you need chunks to span multiple files, you will have to re-chunk the data using the `chunk()` method.\n",
    "\n",
    "In this case, we don't actually need chunks to with 122 time steps. We are fine with whatever chunking `xarray` does by default. If we set `chunks = 'auto'`, then `xarray` will choose to load all the input files into memory at once; hence, there is one chunk per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fc6d4b-18a9-40c2-9355-75fb79071926",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset('./data_raw/MERRA2/*2024*.nc4', chunks = 'auto')\n",
    "ds['T2MMEAN'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185548a-b91f-4047-8cf8-e8fe7a8df1f2",
   "metadata": {},
   "source": [
    "&#x1F449; **Notice how fast each of the above code blocks was executed.** That's because `xarray` hasn't actually loaded any data into memory yet. Remember **lazy evaluation?** Again, `xarray` will not load data into memory until the last minute, when it's absolutely necessary to do so. And `xarray` doesn't need to load data into memory in order to give us the information we were looking at above. It just read a little bit of information from each file to learn how to *represent* the complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c41e0-cb5b-4f28-9e1f-c2286fd65e27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Computing PET using Hargreaves equation\n",
    "\n",
    "In order to calculate the Precipitation-to-PET ratio, we'll first need to use the Hargreaves equation to calculate PET:\n",
    "$$\n",
    "\\text{PET} = 0.0023 \\times R_A \\times \\sqrt{T_{max} - T_{min}} \\times (T + 17.8)\n",
    "$$\n",
    "\n",
    "Above, $R_A$ is the top-of-atmosphere (TOA) solar radiation and $T$, $T_{max}$, and $T_{min}$ are the mean, maximum, and minimum temperatures, respectively.\n",
    "\n",
    "#### &#x1F3AF; Best Practice\n",
    "\n",
    "The Hargreaves equation is just complex enough that we need to develop multiple data-processing steps to get to our goal, which is the Precipitation-to-PET ratio for a defined region. This effort will require that we pay attention to several potential pitfalls of computational data science:\n",
    "\n",
    "- Ensuring that processing steps are done in the correct order, so that data structures and/or Python variables are correctly initialized.\n",
    "- Ensuring that measurement units are correct and compatible between different data processing steps.\n",
    "- Documenting each processing step so that we can identify potential errors and so that a third party can verify or reproduce our analysis.\n",
    "\n",
    "A technique from computer science called **decomposition** can help us to plan our analysis. **Decomposition** involves breaking a problem down into a series of independent, manageable steps. We might decompose our problem into these steps:\n",
    "\n",
    "1. Load the required temperature data inputs.\n",
    "2. Calculate top-of-atmosphere (TOA) solar radiation.\n",
    "3. Calculate potential evapotranspiration (PET) using the Hargreaves equation.\n",
    "4. Compute the Precipitation-to-PET ratio.\n",
    "\n",
    "**These ordered steps should help us to organize our workflow in a way that someone else can easily understand.** We've already loaded the required temperature data (Step 1), so let's move on to calculating TOA radiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00cc7f-c2c2-4a1a-b7ae-a3ebf28f6649",
   "metadata": {},
   "source": [
    "### Computing top-of-atmosphere (TOA) radiation\n",
    "\n",
    "Here is a function for calculating TOA radiation, [based on FAO guidance.](https://www.fao.org/4/X0490E/x0490e07.htm#radiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea17cf3-de5e-45c0-9fa1-eb2bf4ccf171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def toa_radiation(latitude, doy):\n",
    "    '''\n",
    "    Top-of-atmosphere (TOA) radiation for a given latitude (L) and day of year\n",
    "    (DOY) can be calculated as:\n",
    "\n",
    "    R = ((24 * 60) / pi) * G * d * (w * sin(L) * sin(D) + cos(L) * cos(D) * sin(w))\n",
    "\n",
    "    Where G is the solar constant, 0.0820 [MJ m-2 day-1]; d is the (inverse) \n",
    "    relative earth-sun distance; w is the sunset hour angle; and D is the solar\n",
    "    declination angle.\n",
    "    \n",
    "    For more information, consult the FAO documentation:\n",
    "\n",
    "        https://www.fao.org/4/X0490E/x0490e07.htm#radiation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latitude : float\n",
    "        The latitude on earth, in degrees, where southern latitudes\n",
    "        are represented as negative numbers\n",
    "    doy : int\n",
    "        The day of the year (DOY), an integer on [1,366]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Number\n",
    "        Top-of-atmosphere (TOA) radiation, in [MJ m-2 day-1]\n",
    "    '''\n",
    "    assert isinstance(doy, int) or issubclass(doy.dtype.type, np.integer), 'The \"doy\" argument must be an integer'\n",
    "    assert np.all(doy >= 1) and np.all(doy <= 366), 'The \"doy\" argument must be between 1 and 366, inclusive'\n",
    "    \n",
    "    solar_constant = 0.0820 # [MJ m-2 day-1]\n",
    "    pi = 3.14159\n",
    "    \n",
    "    # Convert latitude from degrees to radians\n",
    "    latitude_radians = np.deg2rad(latitude)\n",
    "    # Inverse Earth-Sun distance (relative), as a function of day-of-year (DOY)\n",
    "    earth_sun_dist = 1 + 0.0033 * np.cos((doy * 2 * pi) / 365)\n",
    "    # Solar declination, as a function of DOY\n",
    "    declination = 0.409 * np.sin(((doy * 2 * pi) / 365) - 1.39)\n",
    "    \n",
    "    # Sunset hour angle; we use np.where() below to guard against\n",
    "    #   warnings where arccos() would return invalid values, which\n",
    "    #   happens when the argument is outside [-1, 1]\n",
    "    _hour_angle = -np.tan(latitude_radians) * np.tan(declination)\n",
    "    _hour_angle = np.where(np.abs(_hour_angle) > 1, np.nan, _hour_angle)\n",
    "    sunset_hour_angle = np.arccos(_hour_angle)\n",
    "\n",
    "    # Incident radiation, depends only on the relative earth-sun distance\n",
    "    inc_radiation = ((24 * 60) / pi) * solar_constant * earth_sun_dist\n",
    "    return inc_radiation * (sunset_hour_angle * np.sin(latitude_radians) * np.sin(declination) +\n",
    "            np.cos(latitude_radians) * np.cos(declination) * np.sin(sunset_hour_angle))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec80467-d6a5-458c-9ca6-ea653715d1a4",
   "metadata": {},
   "source": [
    "### Well-documented functions\n",
    "\n",
    "**There are several things to note about this function.**\n",
    "\n",
    "There is a **function-level docstring** that provides rich information about the purpose and use of the function. In addition to the important \"Parameters\" and \"Return\" value sections, we have provided a simple, human-readable form of the equation we're using to calculate TOA radiation. We also provided a link to the FAO document where this equation came from. These are all very important things to include so that someone else can figure out how we're calculating TOA radiation. These things also help us to later verify that we're performing calculations correctly.\n",
    "\n",
    "In the **Parameters** section, we made sure to define the measurement units required for each input parameter. This is *extremely* important. In the above example, we would get a different, and incorrect, answer if `latitude` was given in radians instead of degrees. We also indicated the Python **data type,** e.g., `float`. This is also important to include because, when a computation involves the wrong data type, it is often difficult to figure out that the error is due to an incorrect data type.\n",
    "\n",
    "**Variable names** are chosen carefully. We use `latitude` instead of a name like `x`, which is too short and could signify multiple things. We also defined a variable `latitude_radians` to distinguish when we are using latitude in radians, as opposed to degrees. While `latitude` could have been written as `latitude_degrees`, we decided to compromise clarity for a shorter name in this case, although clarity is usually most important. Ultimately, there are some subjective choices to be made, but you should consider choosing variable names that communicate the meaning *and* the measurement units of the quantity they represent. If that is hard to, **inline comments** can help to keep track of units, as we did with the inline comment next to `solar_constant`.\n",
    "\n",
    "**Constants** are defined at the top of our function: `pi` and `solar_constant`. While many people might recognize a number like 3.14 as the number pi, defining it as a variable, `pi`, in our function makes this more clear and allows for us to control the precision of this number in one place. In general, constants should be defined only once!\n",
    "\n",
    "**Comments** are used frequently. In particular, where there are complex calculation steps to obtain the `sunset_hour_angle`, we have a long comment above the code to explain what it does. If we need to use intermediate variables in our calculation, we can use less informative variable names, like `_hour_angle`. In Python, variable names that begin with the underscore, `_`, signal to users that the variable is less important or can be ignored.\n",
    "\n",
    "For long calculations, like the `return` value of our function, it can be helpful to break them up into smaller, more meaningful quantities, paying attention to the order of operations. This is why we defined the `inc_radiation` variable. When a calculation can't be broken down into meaningful parts, it can improve readability to break the equation across multiple lines, as we did by creating a line break after a `+` operation.\n",
    "\n",
    "Finally, note that we included **assertions,** using the `assert` keyword, to help ensure that users call this function correctly. Consider what happens when the wrong data type, or an out-of-range value, is provided for the `doy` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202363b-d0c1-425a-bfa3-f8d0b23568d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_radiation(36.1, doy = 14.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55ad2f-c967-4589-97a4-6743c7b860c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_radiation(36.1, doy = 500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f4f8c7a-5e09-439e-b177-45ebadaa5816",
   "metadata": {},
   "source": [
    "#### &#x1F3C1; Challenge: Writing a well-documented function\n",
    "\n",
    "Now that we've reviewed what makes a well-documented function, **write the function for the next step of our analysis.** The equation below can be used to calculate PET. Write a well-documented Python function called `potential_et()` that returns PET in units of millimeters per day (mm day$^{-1}$).\n",
    "\n",
    "$$\n",
    "\\text{PET} = 0.0023 \\times R_A \\times \\sqrt{T_{max} - T_{min}} \\times (T + 17.8)\n",
    "$$\n",
    "\n",
    "The inputs to the `potential_et()` function are:\n",
    "\n",
    "- $R_A$ is the top-of-atmosphere solar radiation, in mm H$_2$O equivalent per month\n",
    "- $T_{max}$ is the monthly maximum temperature, in degrees C\n",
    "- $T_{min}$ is the monthly minimum temperature, in degrees C\n",
    "- $T$ is the monthly average temperature, in degrees C\n",
    "\n",
    "**Hint:** There is an `np.sqrt()` function for calculating square roots.\n",
    "\n",
    "Start with editing the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1f925-97ff-47cb-a529-9da174af55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_et(toa_radiation, temp_max, temp_min, temp_mean):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84503fd7-728c-447e-b4cc-b160a020af89",
   "metadata": {},
   "source": [
    "Expand the cell below to see one solution to this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a256345-d59b-4f4d-97a3-ad98748b9d28",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def potential_et(toa_radiation, temp_max, temp_min, temp_mean):\n",
    "    '''\n",
    "    Calculates potential evapotranspiration, according to the Hargreaves\n",
    "    equation:\n",
    "\n",
    "    PET = 0.0023 * R * sqrt(Tmax - Tmin) * (Tmean + 17.8)\n",
    "\n",
    "    Where R is the top-of-atmosphere (TOA) radiation (mm month-1); Tmax and \n",
    "    Tmin are the maximum and minimum monthly air temperatures (degrees C),\n",
    "    respectively; and Tmean is monthly mean air temperature (degrees C).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    toa_radiation : Number\n",
    "        The top-of-atmosphere (TOA) radiation (mm day-1)\n",
    "    temp_max : Number\n",
    "        Maximum monthly air temperature (degrees C)\n",
    "    temp_min : Number\n",
    "        Minimum monthly air temperature (degrees C)\n",
    "    temp_mean : Number\n",
    "        Average monthly air temperature (degrees C)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Number\n",
    "        The potential evapotranspiration (PET) in [mm day-1]\n",
    "    '''\n",
    "    return 0.0023 * toa_radiation * np.sqrt(temp_max - temp_min) * (temp_mean + 17.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad1588-dc1d-458f-a39f-87148ae73ee0",
   "metadata": {},
   "source": [
    "If the function is written correctly, when it is called with the arguments below, you should get a value close to `3.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b387af1c-e65a-46a2-9580-75f6037082d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_et(10, 30, 20, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956e0250-3751-43a6-8c1f-5f677a618997",
   "metadata": {},
   "source": [
    "### Vectorized functions\n",
    "\n",
    "Let's review how to call the Python functions we wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef552f-545a-4dce-b500-1be8a563b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_radiation(32, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad75162-fd7c-4ddd-8ff7-70a5561a39cb",
   "metadata": {},
   "source": [
    "Recall that, because NumPy arrays are treated just like numbers, we can call the `toa_radiation()` function with an array of numbers for one of the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2c6bf-1d1a-4bb0-ab55-eafb461f9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = np.array([22, 32, 42])\n",
    "\n",
    "toa_radiation(lats, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf2fa1-c639-4295-84b9-1761fb8f6a08",
   "metadata": {},
   "source": [
    "In computer science, using a function in this way is called **vectorization** and functions that are compatible with both a single number, as in `toa_radiation(32, 200)`, or an array of numbers, as in `toa_radiation(lats, 200)`, are called **vectorized functions.** Because we are almost always working with arrays of data, rather than single numbers, **vectorized functions** are very important.\n",
    "\n",
    "For example, vectorization allows us to plot the TOA radiation as a function of a range of dates. The latitude value remains the same for each value of `doy`, so that mathematical operations that depend both on day-of-year and latitude look something like adding or multiplying a single number (latitude) to an array of numbers (days of the year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e2598-6ba5-49e0-aca9-b18dd34fdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "doy = np.arange(1, 366).astype(np.int32)\n",
    "\n",
    "rad = toa_radiation(32, doy)\n",
    "pyplot.plot(doy, rad, 'k-')\n",
    "pyplot.xlabel('Day of Year')\n",
    "pyplot.ylabel('TOA Radiation [MJ m-2 day-1]')\n",
    "pyplot.title('TOA Radiation at 32 deg N latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b012e677-e270-40a5-b4e6-4ed5675c94d4",
   "metadata": {},
   "source": [
    "**However, if we have two or more input arrays, they need to be compatible.** Below, we get an error because we're trying to multiple two arrays with incompatible shapes. While `lats` has only 3 elements, `doy` has 365 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1310e-62f1-43b9-a999-e891c2c8deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't work because array shapes are incompatible\n",
    "toa_radiation(lats, doy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f5753-038f-465c-80c5-a5d5d9aee9b8",
   "metadata": {},
   "source": [
    "The only way to fix this without changing our function is to make our two arrays compatible. We could reshape them so that they are 2-dimensional, $T\\times N$ arrays, where $T$ is the number of days in `doy` and $N$ is the number of different latitudes in `lats`. This works well but may be confusing in some cases, because now we have a much larger, 2-dimensional array to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269279d3-ecf6-4002-8864-56ac4c4bce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_radiation(lats.reshape((1,3)), doy.reshape((365,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea6569-67f2-430f-a6de-31442dc93869",
   "metadata": {},
   "source": [
    "### Deriving variables from `xarray` coordinates\n",
    "\n",
    "Vectorization will be key to computing PET for our gridded temperatures dataset, because we have multiple latitudes and multiple days-of-the-year to consider when computing TOA radiation. Consider our dataset's coordinates, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1da3e-95c2-46c0-90c4-426eb5eaa329",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ba724-c6ee-46da-abc0-310b417775a0",
   "metadata": {},
   "source": [
    "**We need to derive:**\n",
    "\n",
    "- The latitude for every pixel\n",
    "- The day-of-year for every pixel\n",
    "\n",
    "The MERRA-2 temperatures dataset has 361 latitude bins represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb97188-d6d7-4a8c-a13f-1bdc81d617fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5dbdc-ca2f-457a-a3a1-5698e4c58ba5",
   "metadata": {},
   "source": [
    "One step towards vectorization is to convert our 1-dimensional array of latitudes into a *grid* of latitudes, since every pixel must have a latitude value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ec0b2-b1d2-4a8c-aeb0-22a002634540",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = ds['lat'].values\n",
    "lats = lats.reshape((361, 1)).repeat(ds['lon'].size, axis = 1)\n",
    "lats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f959ae-66d4-4134-99c0-ba3f53832d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(lats)\n",
    "pyplot.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0f5b2-97ff-400e-a228-67cfb6234836",
   "metadata": {},
   "source": [
    "#### &#x1F6A9; <span style=\"color:red\">Pay Attention</red>\n",
    "\n",
    "Our latitudes seem to be upside-down! Making a plot like the one above is a great way to check our intution.\n",
    "\n",
    "**However, this is not a problem.** Recall that, in our `xarray.Dataset`, the `lats` coordinates go from -90 (south latitude) to +90 (north latitude). We want our latitudes grid to match the coordinates of the existing `xarray.Dataset` because that is how the data variables are also structured. We can verify this by looking at the raw `values` of one of those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e702ea3-9157-44aa-b6a7-46f9f6db8311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.imshow(ds['T2MMIN'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ceaf0-2993-4af5-b5a3-9e0a59e2ea6c",
   "metadata": {},
   "source": [
    "**Hence, when we add the latitude grid to our `xarray.Dataset` as a new variable, we'll want to tell `xarray` what the corresponding axes of the grid are.** Below, we do this by providing assigning a *tuple* to the new `\"lat_grid\"` variable:\n",
    "\n",
    "- The first element of the tuple is another tuple, `('lat', 'lon')` that specifies the order and name of the axes. These should be axes that already exist in the `ds` Dataset.\n",
    "- The second element, `lats`, is our grid of latitudes.\n",
    "\n",
    "Below, the new `\"lat_grid\"` variable is shown as having `(lat, lon)` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815ad09-43bd-4b31-b6bf-c6928056e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lat_grid'] = (('lat', 'lon'), lats)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd4ad7-7153-495f-ad01-b3aa97fc35d3",
   "metadata": {},
   "source": [
    "However, the other data variables (e.g., `T2MMIN`, `T2MMAX`) have `(time, lat, lon)` dimensions. It will be much easier to do some computations later if `\"lat_grid\"` has the same dimensions as all the other data variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7824b5-9710-4ab0-ab04-bf24463f9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats2 = lats.reshape((1, 361, 576)).repeat(122, axis = 0)\n",
    "lats2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68812f-5f82-470a-ad5e-31b63b231aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lat_grid'] = (('time', 'lat', 'lon'), lats2)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28aa40a-c860-4b44-8417-ed74b7f649b6",
   "metadata": {},
   "source": [
    "**Now we have latitude for every pixel. What about day-of-year?** Fortunately, this is easy to derive from our `time` coordinates because they are represented as the `xarray` `datetime64[ns]` data type and therefore have date-time components like `'month'` and `'dayofyear'`. [You can read more about date-time components here.](https://docs.xarray.dev/en/stable/user-guide/time-series.html#datetime-components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e61f52-57b4-4e53-816f-154594e76af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['time.dayofyear']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdce71-53de-4fe7-a125-357b63ee02ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Applying a function to independent chunks\n",
    "\n",
    "Now that we have the data needed to compute TOA radiation, let's test our `toa_radiation()` function using data from a single date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cd204-66f1-4f07-872f-fe4f00ccdfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds.sel(time = '2024-05-01')\n",
    "\n",
    "rad = toa_radiation(test['lat_grid'], test['time.dayofyear'])\n",
    "rad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f435de-2948-49b0-99e4-0eefac4de2d2",
   "metadata": {},
   "source": [
    "Our result has a shape of `(1, 361, 576)` because our input `\"lat_grid\"` and `\"time.dayofyear\"` data arrays have that same shape.\n",
    "\n",
    "We could easily add this result to our `xarray.Dataset`, `test`, using assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883301ed-1ec5-41c9-9811-c7e758790494",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['toa_radiation'] = rad\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6e4bd-794e-4ab3-b444-85262cd9ecd1",
   "metadata": {},
   "source": [
    "In this case, the variable `rad` is already an `xarray.DataArray` with the proper dimensions. However, it's a good idea to review the more general way to add a data variable to an `xarray.Dataset`, where we specify the names and the order of the dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c3dd6-d364-472d-8713-38e8757b4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: When rad is already an xarray.DataArray, we need to write rad.data\n",
    "test['toa_radiation'] = (('time', 'lat', 'lon'), rad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fcacf-bbf8-4f5f-bb0f-c1e2e23e2a47",
   "metadata": {},
   "source": [
    "Let's plot the result to check if everything makes sense. We can see that TOA radiation is highest in the northern hemisphere, along a latitude band of about 25 N latitude, which makes sense for this time of year (May)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c4272-5916-43a6-a09b-09d4090241f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['toa_radiation'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7f545-0271-4651-a5de-fc79b3a7cb73",
   "metadata": {},
   "source": [
    "**How can we apply what we just did to the entire dataset?** Well, we didn't need to subset the dataset to a single day, as we did above. \n",
    "\n",
    "&#x1F449; **However, if our time-series dataset was very long, we might want to think about how best to take advantage of concurrency.** That is, we know that our problem (computing TOA radiation) is independent over space and time: the result of the calculation doesn't depend on adjacent pixels or adjacent time steps. This is a type of problem referred to as **embarassingly parallel** because, in theory, we could have an indefinitely large number of tasks executing in parallel without affecting the result.\n",
    "\n",
    "Before we try take advantage of concurrency, let's review how `xarray` is currently chunking our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8102471-82f9-4968-9905-4d0dbc7ff4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['lat_grid'].chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84accce2-44b1-4b37-87fe-f6943fd73e06",
   "metadata": {},
   "source": [
    "Whoops. Our new `\"lat_grid\"` dataset doesn't actually have any chunks. Let's go ahead and make some. The simplest approach would be to let each time step (each day) be a different chunk. The `'lat'` and `'lon'` dimensions can be set to `'auto'` to indicate we don't care how `xarray` chunks them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ccf92-1ee8-439d-8abb-c4371120357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the 'time' axis in each chunk should be 1: a different chunk for each time step\n",
    "ds['lat_grid'] = ds['lat_grid'].chunk({'lat': 'auto', 'lon': 'auto', 'time': 1})\n",
    "ds['lat_grid']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817af96-3227-4ccf-a956-6892bccae7ec",
   "metadata": {},
   "source": [
    "### Mapping an arbitrary function\n",
    "\n",
    "Now we're ready to apply a function to our independent chunks. With **concurrent processing,** we call this **mapping** a function over independent subsets of our data. In `xarray`, those independent subsets are called either **chunks** or **blocks** (confusingly, `xarray` uses both terms to refer to the same thing). Hence, [the function we want to use to assign a task to independent chunks is called `xarray.map_blocks()`.](https://docs.xarray.dev/en/stable/generated/xarray.map_blocks.html)\n",
    "\n",
    "Consider the example below, where we have an arbitrary function called `my_function()`. This function takes an `xarray.Dataset` as its single argument and returns a modified version of the `\"lat_grid\"` variable.\n",
    "\n",
    "When we map `my_function()` over the independent blocks (chunks), the result is clearly a data cube with the same shape and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54615e7-6052-4b3a-bb17-f822d986d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trivial function that multiplies latitudes by two\n",
    "def my_function(dataset):\n",
    "    return dataset['lat_grid'] * 2\n",
    "\n",
    "xr.map_blocks(my_function, ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff12db-8a3a-42c9-b8a1-31ed3f10aa0b",
   "metadata": {},
   "source": [
    "Again, note how fast that code was executed. Rememeber that `xarray` uses **lazy evaluation** and nothing has happened yet. We need to call the `compute()` method to tell `xarray` we're ready to actually load the data into memory and perform our computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880410e3-0c16-4871-b5d3-a1ce15cfbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = xr.map_blocks(my_function, ds).compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2764ae3-becd-47cd-882c-5085836a8437",
   "metadata": {},
   "source": [
    "Now let's consider our `toa_radiation()` function. That function doesn't know anything about `xarray` variables; it assumes we are calling it with numbers or `numpy` arrays. So, we need to first create a function that wraps `toa_radiation()`, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5b601-b0c2-4541-93d2-cc8cba250af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toa_radiation_wrapper(dataset):\n",
    "    return toa_radiation(dataset['lat_grid'], dataset['time.dayofyear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf506582-9de4-4576-9231-73e87ad9a0a9",
   "metadata": {},
   "source": [
    "Again, we need to follow `map_blocks()` with `compute()` if we are ready to do the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4bddb7-0a22-4656-84c6-2973035b3313",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = xr.map_blocks(toa_radiation_wrapper, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce46b5-55b1-4f84-86db-d75ab90667c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_data = result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28391b-f90c-436c-8fdf-3036492467ba",
   "metadata": {},
   "source": [
    "If we plot the same time step as before, we can see that we got the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf0f5c-0146-4696-8d40-ec34b5717d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "toa_data.sel(time = '2024-05-01').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e9c3c-c4ab-4f54-8fa1-214f92ddf8ea",
   "metadata": {},
   "source": [
    "&#x1F449; One last thing we need to do is a unit conversion. $R_A$ should be multiplied by 0.408 to convert it from [MJ m-2 day-1] to [mm day-1]. Let's go ahead and store this result in our `xarray.Dataset` as well, calling the variable `'toa_radiation'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07dc536-b8f9-4ce6-9fb3-d819e2629c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting TOA Radiation from [MJ m-2 day-1] to [mm H2O day-1]\n",
    "ds['toa_radiation'] = toa_data * 0.408\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7705f2-dbf1-41cb-8e3c-ea9da489e841",
   "metadata": {},
   "source": [
    "#### &#x1F3AF; Best Practice\n",
    "\n",
    "**We now have all the data we need to compute PET, on a consistent global grid, as a single `xarray.Dataset`! Let's make sure to include some field-level metadata, in case we end up sharing this dataset with others.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0426b55-ba92-40fc-acd1-d8eb6c1ec2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['toa_radiation'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45ccdd-4d43-4278-b87e-320601618c77",
   "metadata": {},
   "source": [
    "There are currently no attributes. We should at least make one that clarifies the measurement units of our `'toa_radiation'` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9463521a-5361-4df9-b56d-be3064e17d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['toa_radiation'].attrs['units'] = 'mm H2O day-1'\n",
    "ds['toa_radiation'].attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd561f29-331f-4324-b1e7-e4991edee395",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Profiling computational resources\n",
    "\n",
    "Combining `xarray` and `dask` together makes it easy to use concurrent processing. However, computers are still pretty fast, and there may be several tasks we want to complete that run very quickly on a single CPU, even if the dataset is large. Concurrent processing may be unnecessary.\n",
    "\n",
    "#### &#x1F3AF; Best Practice\n",
    "\n",
    "In general, we should **profile** the resource requirements of a computational workflow before we implement concurrent processing or some other solution. How long does it take to run? How much memory does it require? We should answer these questions first instead of assuming it will take too long, or it won't fit into memory.\n",
    "\n",
    "In this case, because we're still learning, we're using a dataset that is small enough to fit into memory. We know this because, when we open the dataset using `open_mfdataset()`, `xarray` will always show us how much memory each `xarray.DataArray` requires. Below, we can see that a single MERRA-2 variable requires only about 96.77 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ac993-b235-4c65-b90a-4c65edbf5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['T2MMEAN'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7444a-c05a-446f-a8af-7cebe114efdf",
   "metadata": {},
   "source": [
    "**Our task is clearly not limited by memory size; that is, it's not *memory-bound.*** But is it CPU-bound? One way we can answer that question is to time how long it takes to run on a small part of the dataset. For example, let's try running a single day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d07a0-6ec0-4211-af13-c7521cfecab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note there is exactly one chunk; i.e., the subsequent computation will not use more than one process\n",
    "first_day = ds.sel(time = '2024-01-01')\n",
    "first_day['T2MMEAN'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7557c-bb56-4721-9f04-2c640b1ececf",
   "metadata": {},
   "source": [
    "Below, we've re-written the `potential_et()` function so that it works with a MERRA-2 `xarray.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea403c0-cd7b-48a3-ac17-7ebbfaa1bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_et(dataset):\n",
    "    '''\n",
    "    Calculates potential evapotranspiration, according to the Hargreaves\n",
    "    equation:\n",
    "\n",
    "    PET = 0.0023 * R * sqrt(Tmax - Tmin) * (Tmean + 17.8)\n",
    "\n",
    "    Where R is the top-of-atmosphere (TOA) radiation (mm month-1); Tmax and \n",
    "    Tmin are the maximum and minimum monthly air temperatures (degrees C),\n",
    "    respectively; and Tmean is monthly mean air temperature (degrees C).\n",
    "\n",
    "    Single input argument should be an xarray.Dataset with the following\n",
    "    data variables:\n",
    "\n",
    "        T2MMIN: Maximum monthly air temperature (degrees C)\n",
    "        T2MMAX: Minimum monthly air temperature (degrees C)\n",
    "        T2MMEAN: Average monthly air temperature (degrees C)\n",
    "        toa_radiation: The top-of-atmosphere (TOA) radiation (mm day-1)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: xarray.Dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Number\n",
    "        The potential evapotranspiration (PET) in [mm day-1]\n",
    "    '''\n",
    "    return 0.0023 * dataset['toa_radiation'] * np.sqrt(dataset['T2MMAX'] - dataset['T2MMIN']) * (dataset['T2MMEAN'] + 17.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5983dfa-ff6c-4c9d-b366-4b7542270f59",
   "metadata": {},
   "source": [
    "### Measuring the wall time of a task with `timeit`\n",
    "\n",
    "We can use Python's built-in `timeit` module to time how long it takes to run a block of code. `timeit` will report the **wall time** for our task: the time that would be elapsed by a wall clock while the task is running. The **wall time** is usually what we're interested in when we are evaluating how long it will take to complete a task and whether it is worth trying to divide that task among multiple (concurrent) processes.\n",
    "\n",
    "&#x1F449; If we want to use `timeit` inside a Jupyter Notebook, we can just add the magic command `%%timeit` to the top of the Python code block we want to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76279268-d094-4475-9e55-1c1c406a873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "potential_et(first_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46569b-0a93-456f-8795-0025a4bae46a",
   "metadata": {},
   "source": [
    "**But wait!** Remember that `xarray` uses **lazy evaluation.** We need to add a call to `compute()` to make sure we're actually measuring the processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576d65c-5092-4102-92bd-96fcd712df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# TODO Note that we shouldn't try to assign any variables inside a timeit block\n",
    "potential_et(first_day).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc29e9-e484-45eb-86fe-954bcde4b823",
   "metadata": {},
   "source": [
    "[The `timeit` module](https://docs.python.org/3/library/timeit.html) will actually run the code cell multiple times to get an average of how long it took each time. The time it takes will be different for different machines but, on at least one machine we tried this on, it took about 20 milliseconds (20 ms).\n",
    "\n",
    "**If we want to know how long it would take to run `potential_et()` on the entire dataset using a single process,** we can make an initial esitmate by multiplying the number of chunks (number of time steps) by the time it took to run a single chunk, `first_day`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbbe38d-b5f7-4b67-9b20-9c42ea052969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO About 700 ms for a single day\n",
    "\n",
    "20e-3 * ds.time.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c1c98-ef02-4dfe-9694-46de3129fe42",
   "metadata": {},
   "source": [
    "So, it would take only about 2.4 seconds. Because we're working with a smaller dataset for teaching purposes, and because `potential_et()` is still a relatively simple function, processing the entire dataset will be fast.\n",
    "\n",
    "Let's run `potential_et()` over the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af52a1c-2b9a-4a0c-97a1-8b6d3f910209",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "potential_et(ds).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cc3d5-9265-42fa-b09b-ad3f20666f3e",
   "metadata": {},
   "source": [
    "&#x1F449; **Actually, it took a lot less than 2.4 seconds to run `potential_et()` over the entire dataset. Why?** Our estimate based on running a single chunk will always be an overestimate because software like `xarray` and the Python interpreter itself will often figure out ways to optimize a task that is executed in a loop, which is essentially what is happening when we run `potential_et()` over multiple chunks using a single CPU.\n",
    "\n",
    "Now, let's compare the single-CPU wall time to a multiple-CPU wall time, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18503d8c-08b4-4c9e-a830-8a5957bf1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "xr.map_blocks(potential_et, ds).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36aa940-9cd0-4c8c-910d-8a4d69a25560",
   "metadata": {},
   "source": [
    "&#x1F449; **Now our task runs even slower! What happened?** This is because, when we use multiple CPUs or **concurrent processes,** there is a certain amount of overhead associated with scheduling a task and collecting the results. Some of that overhead might include multiple processes (CPUs) waiting on their turn to read or write data, as one example. When our task, `potential_et()`, is already quite fast (as we saw above), this overhead can be significant compared to the time it actually takes to run the task.\n",
    "\n",
    "Multi-process overhead can also be significant when there is a large number of chunks, because the scheduler has more work to do in coordinating the workload for each process. **Selecting the right chunk size is a balance.** If chunks are too large, there won't be much of a decrease in **wall time** associated with **concurrent processing** because fewer processes are being used. Memory might also be an issue with large chunks. But if chunks are too small, then processes (CPUs) will often be waiting for new data.\n",
    "\n",
    "[Read more about chunk size and concurrency here.](https://docs.xarray.dev/en/stable/user-guide/dask.html#chunking-and-performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adbb445-d4ea-45f1-aa4d-f14725adf3e5",
   "metadata": {},
   "source": [
    "#### &#x1F6A9; <span style=\"color:red\">Pay Attention</red>\n",
    "\n",
    "**When we use the `%%timeit` magic command in a code cell, it can interfere with variable assignment.** In general, we should only use `%%timeit` to estimate the wall time. If we wanted to get the *result* of a computation, we should run it in a code cell *without* the `%%timeit` magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e45e9-2ea4-48a5-a0d7-808331b9f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = potential_et(ds).compute()\n",
    "\n",
    "# If we applied a custom function, the result might inherit its name from\n",
    "#   an existing input variable; we can reset the name this way\n",
    "result.name = 'Potential ET (mm day-1)'\n",
    "result.sel(time = '2024-01-01').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcfd01b-e8e5-4116-b592-af0bb569d8dc",
   "metadata": {},
   "source": [
    "Read more about the `timeit` module here:\n",
    "\n",
    "- https://docs.python.org/3/library/timeit.html\n",
    "- https://sjvrijn.github.io/2019/09/28/how-to-timeit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3b55a-8b90-4a4e-81cf-d81ad36dea69",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af658ce2-d186-4bcb-b3ae-0063d9de63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pet = potential_et(ds)\n",
    "pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5d88c-94c9-4281-9538-834613af454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_tiaret = pet.sel(lon = -1.32, lat = 35.37, method = 'nearest')\n",
    "pet_tiaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918033a7-bb7e-4944-88d2-f6738e6e18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_tiaret.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567d5a0-7786-43ec-aad9-a80b41019809",
   "metadata": {},
   "outputs": [],
   "source": [
    "chirps = xr.open_mfdataset('data_raw/CHIRPS/CHIRPS-v2_Africa_monthly_2014-2024.nc')\n",
    "chirps_tiaret = chirps['precip'].sel(x = slice(0.8, 1.8), y = slice(36.1, 35.1))\n",
    "chirps_tiaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45094f3a-246a-4d47-a7cb-0bf13d7c42f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Increasing the frequency of our monthly dataset to daily using nearest-neighbor interpolation\n",
    "\n",
    "chirps_tiaret_resampled = chirps_tiaret.isel(time = slice(120, 125)).resample(time = 'D').nearest()\n",
    "chirps_tiaret_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6df090-9efe-4e9f-85e8-b996e3d4df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Note that we're using a rough approximation of the number of days in a month\n",
    "\n",
    "chirps_tiaret_daily = chirps_tiaret_resampled.mean(['x', 'y']) / 30\n",
    "chirps_tiaret_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f5a3c-2bac-4556-9edf-bdc76624542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = chirps_tiaret_daily.values / pet_tiaret.values\n",
    "\n",
    "pyplot.figure(figsize = (12, 4))\n",
    "pyplot.plot(pet['time'].values, ratio, 'k-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b622b544-43ed-46ff-a557-d6977e9ddf99",
   "metadata": {},
   "source": [
    "On its own, the graph above doesn't tell us how severe the drought in Tiaret is. Although precipitation in the region has replenished less than 5% of its lost water over the past few months, this could be part of the normal seasonal cycle. Actually, we know that January through April is a relatively wet period for Tiaret, but the question remains: **Can we compare this year to past years?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f598c-bfc5-4c82-9811-648cda5de0bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- When using **concurrent processing,** the chunk size must be selected carefully. Too many small chunks may result in a lot of overhead. With too much overhead, a task might run slower with multiple processes than with a single process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904ee41-a9c3-4a5f-b4a4-ab54a9f9c523",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### More resources\n",
    "\n",
    "- The National Center for Atmospheric Research (NCAR) has an excellent article on [\"Using `dask` to scale up your data analysis.\"](https://ncar.github.io/Xarray-Dask-ESDS-2024/notebooks/02-dask-intro.html)\n",
    "- [Parallel computing with `dask`](docs.xarray.dev/en/stable/user-guide/dask.html)\n",
    "- Sander van Rijn's [tutorial on using the `timeit` module.](https://sjvrijn.github.io/2019/09/28/how-to-timeit.html)\n",
    "\n",
    "### References\n",
    "\n",
    "Bohr, Mark. 2007. \"A 30-year retrospective on Dennard's MOSFET scaling paper.\" [https://www.eng.auburn.edu/~agrawvd/COURSE/READING/LOWP/Boh07.pdf](https://www.eng.auburn.edu/~agrawvd/COURSE/READING/LOWP/Boh07.pdf)\n",
    "\n",
    "Moore, Gordon E. 1965. \"Cramming more components onto integrated circuits\" *Electronics Magazine.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
